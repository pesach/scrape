# Backblaze B2 Configuration
B2_KEY_ID=your_backblaze_key_id
B2_APP_KEY=your_backblaze_app_key
B2_BUCKET_NAME=your_bucket_name

# Supabase Configuration
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_ANON_KEY=your_supabase_anon_key

# Optional: HTTP/HTTPS Proxy
HTTP_PROXY=
HTTPS_PROXY=
# YouTube Video Scraper Configuration
# Copy this file to .env and fill in your values

# ====================
# REQUIRED CONFIGURATION
# ====================

# Supabase Configuration
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your_supabase_anon_key

# Backblaze B2 Configuration
B2_APPLICATION_KEY_ID=your_b2_key_id
B2_APPLICATION_KEY=your_b2_application_key
B2_BUCKET_NAME=your_bucket_name
B2_ENDPOINT_URL=https://s3.us-west-004.backblazeb2.com

# Redis Configuration (for Celery task queue)
REDIS_URL=redis://localhost:6379/0

# ====================
# OPTIONAL CONFIGURATION
# ====================

# Application Settings
DOWNLOAD_PATH=/tmp/youtube_downloads
MAX_FILE_SIZE_GB=5
ENVIRONMENT=development
DEBUG=false

# ====================
# HUMAN-LIKE SCRAPING SETTINGS
# ====================

# Realistic browser headers
SCRAPER_USER_AGENT=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36
SCRAPER_ACCEPT_LANGUAGE=en-US,en;q=0.9

# Cookies (choose one method)
# Option 1: Path to Netscape format cookies.txt file
# YT_COOKIES_FILE=/path/to/cookies.txt

# Option 2: Extract cookies from browser profile
# COOKIES_FROM_BROWSER=chrome  # Options: chrome, firefox, brave, edge

# Watch-time pacing
SIMULATE_WATCH_TIME=false  # Set to true to download at watch speed
WATCH_SPEED=1.25           # 1.0 = realtime, >1 = faster than realtime
HUMAN_DELAY_MIN_SEC=3.0    # Minimum delay between videos (seconds)
HUMAN_DELAY_MAX_SEC=10.0   # Maximum delay between videos (seconds)

# Optional hard cap on download rate (bytes/sec)
# If set, overrides watch-time derived rate
DOWNLOAD_RATELIMIT_BPS=0

# ====================
# SCRAPERAPI CONFIGURATION (OPTIONAL)
# ====================

# ScraperAPI provides an alternative to yt-dlp for metadata extraction
# with automatic proxy rotation, CAPTCHA solving, and JavaScript rendering

# Enable ScraperAPI for YouTube scraping
USE_SCRAPERAPI=false

# Your ScraperAPI key (get it from https://www.scraperapi.com)
SCRAPERAPI_KEY=your_scraperapi_key_here

# ScraperAPI endpoint (default is usually fine)
SCRAPERAPI_ENDPOINT=https://api.scraperapi.com

# Render JavaScript content (required for YouTube)
SCRAPERAPI_RENDER=true

# Use premium proxy pool for better success rates
SCRAPERAPI_PREMIUM=false

# Automatically retry failed requests
SCRAPERAPI_RETRY_FAILED=true

# Request timeout in seconds
SCRAPERAPI_TIMEOUT=60

# ====================
# NOTES
# ====================

# 1. ScraperAPI is optional - the system works fine with just yt-dlp
# 2. ScraperAPI only extracts metadata; video downloads still use yt-dlp
# 3. If ScraperAPI fails, the system automatically falls back to yt-dlp
# 4. ScraperAPI is useful when experiencing rate limits or IP blocks
# 5. For production, consider using environment variables instead of .env file